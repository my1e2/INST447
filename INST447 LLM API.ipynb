{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "o4f4_U3Y2yM4",
   "metadata": {
    "id": "o4f4_U3Y2yM4"
   },
   "source": [
    "# INST447: Data Sources and Manipulation\n",
    "## Lecture 08: LLM APIs as Data Sources and Processors\n",
    "\n",
    "Today's Topics:\n",
    "- Data augmentation with AI-powered functions\n",
    "- OpenAI and OpenRouter APIs\n",
    "- Practical applications for text analysis\n",
    "- Best practices and cost management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FAZBec9Y2yM5",
   "metadata": {
    "id": "FAZBec9Y2yM5"
   },
   "source": [
    "## Part 1: Motivation - Beyond Static Data Sources\n",
    "\n",
    "### Recap: How We've Augmented Data So Far\n",
    "\n",
    "**Method 1: Merging/Joining DataFrames**\n",
    "- Combining data from multiple tables\n",
    "- Example: Adding customer demographics to purchase data\n",
    "- Limitation: Need pre-existing structured data\n",
    "\n",
    "**Method 2: Applying Functions**\n",
    "- Using .apply() or .map() to transform data\n",
    "- Example: Converting temperatures, calculating totals\n",
    "- Limitation: Functions must be pre-defined\n",
    "\n",
    "**Method 3: Using APIs to Fetch Data (Lecture 07)**\n",
    "- Retrieving external data via web APIs\n",
    "- Example: Getting product info from Open Food Facts\n",
    "- Limitation: Can only get what the API offers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ff7ddb",
   "metadata": {
    "id": "e0ff7ddb"
   },
   "source": [
    "\n",
    "### Today's Challenge: What If We Need Custom Analysis?\n",
    "\n",
    "**Real-World Scenarios:**\n",
    "- ðŸ“Š Analyzing sentiment of 10,000 customer reviews\n",
    "- ðŸ·ï¸ Classifying news articles into custom categories\n",
    "- ðŸ” Extracting named entities (people, places, companies) from text\n",
    "- âœ… Verifying claims or fact-checking statements\n",
    "- ðŸŒ Translating content while preserving context\n",
    "- ðŸ“ Summarizing long documents\n",
    "\n",
    "**Traditional Approaches (and Their Problems):**\n",
    "\n",
    "1. **Local Libraries (NLTK, spaCy, scikit-learn)**\n",
    "   - âœ… Pro: Fast, no internet needed, free\n",
    "   - âŒ Con: Limited capabilities, rigid, often need training data\n",
    "   - âŒ Con: Poor with nuance, context, or novel tasks\n",
    "\n",
    "2. **Custom Machine Learning Models**\n",
    "   - âœ… Pro: Can be highly accurate for specific tasks\n",
    "   - âŒ Con: Requires ML expertise, training data, and infrastructure\n",
    "   - âŒ Con: Time-consuming to develop and maintain\n",
    "\n",
    "3. **Rule-Based Systems**\n",
    "   - âœ… Pro: Deterministic, explainable\n",
    "   - âŒ Con: Brittle, don't handle edge cases or natural language variation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508fa3ea",
   "metadata": {
    "id": "508fa3ea"
   },
   "source": [
    "### The LLM Solution\n",
    "\n",
    "**You already know ChatGPT can handle these tasks!**\n",
    "\n",
    "You could:\n",
    "1. Copy a review into ChatGPT\n",
    "2. Type: \"What's the sentiment of this review?\"\n",
    "3. Copy the response\n",
    "4. Paste into your spreadsheet\n",
    "5. Repeat 10,000 times... ðŸ˜±\n",
    "\n",
    "**The problem:** Manual, slow, not scalable, error-prone\n",
    "\n",
    "**The solution:** LLM APIs let you automate this!\n",
    "- Same AI models (GPT-4, Claude, etc.)\n",
    "- But accessible via code\n",
    "- Process thousands of items automatically\n",
    "- Integrate directly with pandas DataFrames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895140e6",
   "metadata": {
    "id": "895140e6"
   },
   "source": [
    "### Today's Running Example\n",
    "\n",
    "We'll work with customer restaurant reviews and learn to:\n",
    "- Classify sentiment (positive/negative/neutral)\n",
    "- Extract key topics and entities\n",
    "- Identify specific issues (service, food, ambiance)\n",
    "- Generate concise summaries\n",
    "\n",
    "Let's start by setting up our sample dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NSPciBhB2yM6",
   "metadata": {
    "id": "NSPciBhB2yM6"
   },
   "outputs": [],
   "source": [
    "# Setup: Import libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Create sample dataset - restaurant reviews\n",
    "reviews_df = pd.DataFrame({\n",
    "    'review_id': range(1, 11),\n",
    "    'customer_name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve',\n",
    "                     'Frank', 'Grace', 'Henry', 'Iris', 'Jack'],\n",
    "    'review_text': [\n",
    "        \"Absolutely loved this place! The pasta was phenomenal and the service was impeccable.\",\n",
    "        \"Worst experience ever. Cold food, rude staff, and overpriced. Never coming back.\",\n",
    "        \"It was fine. Nothing special, nothing terrible. Just average Italian food.\",\n",
    "        \"Amazing ambiance and the tiramisu was to die for! Will definitely return.\",\n",
    "        \"Overpriced and underwhelming. The portions were tiny and lacked flavor.\",\n",
    "        \"Good food but had to wait 45 minutes for a table despite having a reservation.\",\n",
    "        \"Incredible! Best Italian food I've had outside of Italy. The chef is a genius.\",\n",
    "        \"Mediocre at best. Expected more given all the positive reviews I read.\",\n",
    "        \"The pizza was decent but our waiter was so attentive and friendly.\",\n",
    "        \"Disappointing. The carbonara was bland and the restaurant was too noisy.\"\n",
    "    ],\n",
    "    'date': pd.date_range('2024-01-01', periods=10, freq='D')\n",
    "})\n",
    "\n",
    "print(\"Sample Restaurant Reviews Dataset:\")\n",
    "print(reviews_df)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w1Vqeo7W2yM6",
   "metadata": {
    "id": "w1Vqeo7W2yM6"
   },
   "source": [
    "## Part 2: Understanding LLM APIs\n",
    "\n",
    "### What is an LLM API?\n",
    "\n",
    "**Recall from last week: Traditional APIs**\n",
    "- Request specific, predefined data\n",
    "- Example: \"Give me product info for barcode X\"\n",
    "- Response: Structured data (JSON)\n",
    "- **Function is predefined by the API provider**\n",
    "\n",
    "Example from Lecture 07:\n",
    "```\n",
    "GET https://world.openfoodfacts.org/api/v0/product/3017620425035.json\n",
    "â†’ Returns: Product data (name, nutrients, ingredients, etc.)\n",
    "```\n",
    "\n",
    "**LLM APIs: A Different Paradigm**\n",
    "- Send a text prompt (instruction + data)\n",
    "- LLM generates a response based on understanding\n",
    "- Response: Generated text (may need parsing)\n",
    "- **Function is defined by YOU in the prompt**\n",
    "\n",
    "Example LLM API call:\n",
    "```\n",
    "POST https://api.openai.com/v1/chat/completions\n",
    "Body: {\n",
    "  \"model\": \"gpt-3.5-turbo\",\n",
    "  \"messages\": [{\"role\": \"user\", \"content\": \"Classify sentiment: The food was great!\"}]\n",
    "}\n",
    "â†’ Returns: \"positive\"\n",
    "```\n",
    "\n",
    "**Key Difference:**\n",
    "- Traditional API: A lot of endpoints, each corresponding to a predefined function.\n",
    "- LLM API: Few \"endpoitns\", functions defined by prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1198e2",
   "metadata": {
    "id": "eb1198e2"
   },
   "source": [
    "### Popular LLM API Providers\n",
    "\n",
    "**Closed-Source Models:**\n",
    "\n",
    "**1. OpenAI (GPT Models)**\n",
    "- Models: GPT series (GPT5, GPT5-mini, GPT5-nano), o-series, etc.\n",
    "- Industry standard, best documentation\n",
    "\n",
    "**2. Anthropic (Claude)**\n",
    "- Models: Claude Opus/Sonnet/Haiku\n",
    "- Known for long context and safety\n",
    "\n",
    "**3. Google (Gemini)**\n",
    "- Models: Gemini 2.5 Pro, Flash\n",
    "- Multimodal (text + images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc253cc4",
   "metadata": {
    "id": "fc253cc4"
   },
   "source": [
    "**Open-Source Models:**\n",
    "\n",
    "**1. Meta (Llama)**\n",
    "- Good performance, lighter weight\n",
    "\n",
    "**2. DeepSeek**\n",
    "- Strong reasoning capabilities, cost-effective\n",
    "\n",
    "**3. Alibaba (Qwen)**\n",
    "- Multilingual, good for Asian languages\n",
    "- Multiple size to choose from"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f85e37",
   "metadata": {
    "id": "e4f85e37"
   },
   "source": [
    "**Deployment Options:**\n",
    "\n",
    "- **Cloud APIs:** OpenAI, Anthropic, Google (pay per use)\n",
    "- **OpenRouter:** Gateway to 100+ models with one API key (includes free models!)\n",
    "- **Local deployment:** Run Llama, DeepSeek, Qwen on your own hardware (free, private)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a75dc29",
   "metadata": {
    "id": "8a75dc29"
   },
   "source": [
    "### How LLM APIs Work (Conceptual Flow)\n",
    "\n",
    "1. **You prepare a prompt:**\n",
    "   - Instruction: \"Classify the sentiment of this review\"\n",
    "   - Data: \"The food was amazing!\"\n",
    "   - Format request: \"positive, negative, or neutral\"\n",
    "\n",
    "2. **Send to API:**\n",
    "   - HTTP POST request with your prompt\n",
    "   - Include API key for authentication\n",
    "   - Specify model to use\n",
    "\n",
    "3. **LLM processes:**\n",
    "   - Model analyzes your prompt\n",
    "   - Generates response based on training\n",
    "   - Returns text output\n",
    "\n",
    "4. **You receive and parse:**\n",
    "   - Extract generated text from JSON response\n",
    "   - Parse into desired format (may need cleaning)\n",
    "   - Handle errors if response is unexpected\n",
    "\n",
    "5. **You pay:**\n",
    "   - Cost based on tokens (chunks of text)\n",
    "   - Both input tokens (your prompt) and output tokens (response)\n",
    "   - Example: ~$0.0005 for 1000 tokens on GPT-3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6aee1b0",
   "metadata": {
    "id": "e6aee1b0"
   },
   "source": [
    "### Important Concepts\n",
    "\n",
    "**Tokens:**\n",
    "- Not the same as words\n",
    "- Roughly: 1 token â‰ˆ 4 characters â‰ˆ 0.75 words\n",
    "- Example: \"Hello, world!\" â‰ˆ 4 tokens\n",
    "- Both input and output count toward cost\n",
    "\n",
    "**Models:**\n",
    "- Different models = different capabilities, speed, and costs\n",
    "- GPT5 -> GPT5-mini -> GPT5-nano\n",
    "- Claude Opus -> Claude Sonnet -> Claude Haiku\n",
    "- Choose based on task complexity\n",
    "\n",
    "**Temperature (optional parameter):**\n",
    "- Controls randomness (0 = deterministic, 2 = creative)\n",
    "- For data processing or debugging, usually want low temperature (0-0.3)\n",
    "- For creative tasks, higher temperature (0.7-1.5)\n",
    "> What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. (OpenAI documentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xXfUJ25a2yM7",
   "metadata": {
    "id": "xXfUJ25a2yM7"
   },
   "source": [
    "## Part 3: Making Your First LLM API Call\n",
    "\n",
    "### Getting Set Up (Brief Overview)\n",
    "\n",
    "1. **An API Key**\n",
    "   - OpenAI: Sign up at https://platform.openai.com/, create API key\n",
    "   - OpenRouter: Sign up at https://openrouter.ai/ (has FREE models!)\n",
    "   - Format: `sk-proj-abc123...xyz789`\n",
    "   - âš ï¸ Keep it secret - treat it like a password!\n",
    "\n",
    "2. **Security: Use Environment Variables**\n",
    "   âŒ **NEVER:** `api_key = \"sk-proj-abc123xyz\"  # Hardcoded!`\n",
    "   âœ… **DO:** `api_key = os.getenv('OPENAI_API_KEY')`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35da8b37",
   "metadata": {
    "id": "35da8b37"
   },
   "source": [
    "### Step 3.2: Understanding Message Roles - How Chat Actually Works\n",
    "\n",
    "**LLM APIs use a \"chat\" format with different roles:**\n",
    "\n",
    "**1. System Message (optional but powerful):**\n",
    "   - Sets the behavior/personality of the assistant\n",
    "   - Example: \"You are a helpful data analyst who gives concise answers.\"\n",
    "   - Think of it as \"programming\" the AI's behavior\n",
    "   - Stays consistent across the conversation\n",
    "\n",
    "**2. User Message:**\n",
    "   - Your actual request or question\n",
    "   - Example: \"Classify the sentiment of this review: ...\"\n",
    "   - This is what you're asking the AI to do\n",
    "\n",
    "**3. Assistant Message:**\n",
    "   - The AI's responses\n",
    "   - When building chat history, previous responses go here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb79dd8",
   "metadata": {
    "id": "bbb79dd8"
   },
   "source": [
    "**How chat history works:**\n",
    "\n",
    "**Important:** Each API call is independent! The model doesn't \"remember\" previous calls.\n",
    "\n",
    "To build conversation context, YOU must include previous messages:\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's 2+2?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"4\"},  # YOU add this from previous response\n",
    "    {\"role\": \"user\", \"content\": \"What about 2+3?\"}  # Now AI has context!\n",
    "]\n",
    "```\n",
    "\n",
    "**Key insight:**\n",
    "- Each API call is stateless (no memory between calls)\n",
    "- To maintain context, include previous messages in the array\n",
    "- The model sees what you send, nothing more\n",
    "\n",
    "**For data processing (our focus today):**\n",
    "- We usually use simple system + user messages\n",
    "- No history needed - each review is independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "gdVlEnfx2yM7",
   "metadata": {
    "id": "gdVlEnfx2yM7"
   },
   "outputs": [],
   "source": [
    "# API Key setup\n",
    "# In production: api_key = os.getenv('OPENAI_API_KEY')\n",
    "# For demo, we'll use a burner key.\n",
    "OPENAI_API_KEY = \"your-api-key-here\"  # Replace with your actual key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g7LJn-Tv2yM7",
   "metadata": {
    "id": "g7LJn-Tv2yM7"
   },
   "source": [
    "### Method 1 - Raw API Call with Requests\n",
    "\n",
    "Let's start by making an API call the \"hard way\" using the `requests` library.\n",
    "This helps us understand what's actually happening under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9znG9Qld2yM7",
   "metadata": {
    "id": "9znG9Qld2yM7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 401\n",
      "{\n",
      "    \"error\": {\n",
      "        \"message\": \"Incorrect API key provided: your-api*****here. You can find your API key at https://platform.openai.com/account/api-keys.\",\n",
      "        \"type\": \"invalid_request_error\",\n",
      "        \"param\": null,\n",
      "        \"code\": \"invalid_api_key\"\n",
      "    }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def call_openai_raw(prompt_text, api_key):\n",
    "    url = \"https://api.openai.com/v1/chat/completions\" # OpenAI Chat Completions endpoint\n",
    "\n",
    "    # Request headers\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Request body\n",
    "    data = {\n",
    "        \"model\": \"gpt-5-mini\",  # Fast, cheap, high quality (2025 standard)\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt_text}\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Make the request\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "    # Check if successful\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        return result\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return None\n",
    "\n",
    "prompt = \"What is the capital of France? Answer in one word.\"\n",
    "\n",
    "raw_response = call_openai_raw(prompt, OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4125f82d",
   "metadata": {
    "id": "4125f82d"
   },
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f4fb7a",
   "metadata": {
    "id": "35f4fb7a"
   },
   "outputs": [],
   "source": [
    "print(\"Full API Response:\")\n",
    "print(json.dumps(raw_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3ee60b",
   "metadata": {
    "id": "ef3ee60b"
   },
   "outputs": [],
   "source": [
    "print(\"Just the answer:\")\n",
    "answer = raw_response['choices'][0]['message']['content']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n96pTKRl2yM8",
   "metadata": {
    "id": "n96pTKRl2yM8"
   },
   "source": [
    "**What We See in the Response:**\n",
    "\n",
    "The API returns a large JSON object with:\n",
    "- `id`: Unique identifier for this completion\n",
    "- `object`: Type of object (\"chat.completion\")\n",
    "- `created`: Timestamp\n",
    "- `model`: Which model was actually used\n",
    "- `choices`: Array of generated responses (usually just one)\n",
    "  - `message`: The actual response\n",
    "    - `role`: \"assistant\"\n",
    "    - `content`: **The text we want!**\n",
    "  - `finish_reason`: Why it stopped (\"stop\" = completed naturally)\n",
    "- `usage`: Token counts\n",
    "  - `prompt_tokens`: Tokens in your prompt\n",
    "  - `completion_tokens`: Tokens in the response\n",
    "  - `total_tokens`: Sum (this determines cost!)\n",
    "\n",
    "**To get the actual answer, we navigate:**\n",
    "```python\n",
    "answer = response['choices'][0]['message']['content']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2BfH5lFG2yM8",
   "metadata": {
    "id": "2BfH5lFG2yM8"
   },
   "source": [
    "### Method 2 - Using the OpenAI Wrapper Library\n",
    "\n",
    "OpenAI provides an official wrapper that makes our lives much easier!\n",
    "\n",
    "**Benefits of wrappers:**\n",
    "- Cleaner code\n",
    "- Better error handling\n",
    "- Automatic retries\n",
    "- Type hints and documentation\n",
    "- Less manual JSON parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08K4RaRZ2yM8",
   "metadata": {
    "id": "08K4RaRZ2yM8"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def call_openai_wrapper(prompt_text, api_key):\n",
    "    client = OpenAI(api_key=api_key)  # Initialize the client\n",
    "\n",
    "    # Make the call\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt_text}\n",
    "        ],\n",
    "        temperature=0.3\n",
    "    )\n",
    "\n",
    "    # Extract the answer\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "answer = call_openai_wrapper(prompt, OPENAI_API_KEY)\n",
    "print(f\"Question: {prompt}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6qcIMlHl2yM8",
   "metadata": {
    "id": "6qcIMlHl2yM8"
   },
   "source": [
    "## Part 4: Applying LLM APIs to Real Data\n",
    "\n",
    "### The Core Pattern: DataFrame + AI Analysis\n",
    "\n",
    "**Remember from pandas:**\n",
    "- We can add new columns with calculations: `df['total'] = df['price'] * df['quantity']`\n",
    "- We can apply functions: `df['uppercase'] = df['name'].apply(str.upper)`\n",
    "\n",
    "**Today, we'll do:**\n",
    "```python\n",
    "df['sentiment'] = df['review_text'].apply(analyze_with_llm)\n",
    "```\n",
    "\n",
    "Where `analyze_with_llm` sends each review to an LLM API!\n",
    "\n",
    "### Single Review Analysis\n",
    "\n",
    "Let's start by analyzing just ONE review to understand the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BK7tGT_Z2yM8",
   "metadata": {
    "id": "BK7tGT_Z2yM8"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def analyze_sentiment(text: str) -> dict:\n",
    "\n",
    "    # Set up OpenAI API key\n",
    "    openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a sentiment analysis expert. Analyze the sentiment of the given text and respond with JSON containing: sentiment (positive/negative/neutral), score (0-1), and a brief explanation.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Analyze the sentiment of this text: {text}\"\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    result = eval(response.choices[0].message.content)\n",
    "    return result\n",
    "\n",
    "\n",
    "# Test on a single review\n",
    "sample_review = reviews_df.iloc[0]['review_text']\n",
    "sentiment = analyze_sentiment(sample_review)\n",
    "print(f\"Review: {sample_review}\")\n",
    "print(f\"Sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14eee55",
   "metadata": {
    "id": "c14eee55"
   },
   "outputs": [],
   "source": [
    "sentiment['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wE7zpXg22yM8",
   "metadata": {
    "id": "wE7zpXg22yM8"
   },
   "source": [
    "### Applying to Entire DataFrame\n",
    "\n",
    "Now let's apply sentiment analysis to ALL reviews!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OYyy26gl2yM8",
   "metadata": {
    "id": "OYyy26gl2yM8"
   },
   "outputs": [],
   "source": [
    "# Note: In practice, this would make 10 API calls (one per review)\n",
    "reviews_df['sentiment'] = reviews_df['review_text'].apply(analyze_sentiment)\n",
    "\n",
    "print(\"Reviews with Sentiment Analysis:\")\n",
    "print(reviews_df[['customer_name', 'review_text', 'sentiment']])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bArwV6FJ2yM8",
   "metadata": {
    "id": "bArwV6FJ2yM8"
   },
   "source": [
    "**What just happened?**\n",
    "\n",
    "1. For each row in the DataFrame:\n",
    "   - Extract the review_text\n",
    "   - Call analyze_sentiment() with that text\n",
    "   - LLM analyzes and returns sentiment\n",
    "   - Result stored in new 'sentiment' column\n",
    "\n",
    "2. In real usage with actual API:\n",
    "   - This made 10 API calls (one per review)\n",
    "   - Each call costs ~$0.0001 (tiny, but adds up!)\n",
    "   - Total time: ~10-20 seconds (depending on API speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DGMoWHgl2yM8",
   "metadata": {
    "id": "DGMoWHgl2yM8"
   },
   "source": [
    "### Limitations of .apply() & Production Considerations\n",
    "\n",
    "**The .apply() approach is great for learning, but has limitations for large-scale:**\n",
    "\n",
    "- **Sequential processing:** One at a time (slow for thousands of rows)\n",
    "- **No progress saving:** Crash at row 1500 of 2000 = start over\n",
    "- **No error handling:** One API error crashes everything\n",
    "\n",
    "**For production with large datasets, consider:**\n",
    "- Batch processing with checkpointing (save progress periodically)\n",
    "- OpenAI Batch API (50% cheaper, submit jobs and retrieve later)\n",
    "- Async/concurrent requests (much faster)\n",
    "\n",
    "**For this lecture:** We'll stick with .apply() for simplicity. Just know better tools exist!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Zuetizmz2yM8",
   "metadata": {
    "id": "Zuetizmz2yM8"
   },
   "source": [
    "### Step 4.4: Comparison with Traditional Methods\n",
    "\n",
    "Before we move on, let's compare the LLM approach with a traditional library.\n",
    "\n",
    "**Traditional approach: TextBlob (NLTK-based)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h7_f3xgM2yM9",
   "metadata": {
    "id": "h7_f3xgM2yM9"
   },
   "outputs": [],
   "source": [
    "# Traditional sentiment analysis with TextBlob\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "def sentiment_textblob(text):\n",
    "    \"\"\"Traditional sentiment analysis using TextBlob\"\"\"\n",
    "    score = TextBlob(text).sentiment.polarity\n",
    "    if score > 0.1:\n",
    "        return \"positive\"\n",
    "    elif score < -0.1:\n",
    "        return \"negative\"\n",
    "    return \"neutral\"\n",
    "\n",
    "reviews_df['textblob_sentiment'] = reviews_df['review_text'].apply(sentiment_textblob)\n",
    "print(\"Reviews with TextBlob Sentiment Analysis:\")\n",
    "print(reviews_df[['customer_name', 'review_text', 'textblob_sentiment']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "E-8AjgiZ2yM9",
   "metadata": {
    "id": "E-8AjgiZ2yM9"
   },
   "source": [
    "## Part 5: OpenRouter - Gateway to Many Models\n",
    "\n",
    "**OpenRouter: One API for 100+ models**\n",
    "\n",
    "Instead of signing up for OpenAI, Anthropic, Google separately:\n",
    "- One API key for all providers\n",
    "- Access to free models (Llama, DeepSeek, Qwen, etc.)\n",
    "- Same code structure as OpenAI\n",
    "\n",
    "**How to use:**\n",
    "```python\n",
    "from openai import OpenAI\n",
    "\n",
    "# Just change the base_url!\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.getenv('OPENROUTER_API_KEY')\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/llama-3.1-8b-instruct:free\",  # Free model!\n",
    "    messages=[{\"role\": \"user\", \"content\": \"...\"}]\n",
    ")\n",
    "```\n",
    "\n",
    "**When to use OpenRouter:**\n",
    "- âœ… Want to experiment with free models\n",
    "- âœ… Compare different models easily\n",
    "- âœ… Access models from multiple providers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s4E9GR2L2yM9",
   "metadata": {
    "id": "s4E9GR2L2yM9"
   },
   "source": [
    "## Part 6: Best Practices - Key Things to Remember\n",
    "\n",
    "### Cost Management\n",
    "- **Costs add up:** $0.0001 per review Ã— 10,000 reviews = $1\n",
    "- **Use cheaper models when possible:** GPT-5-mini vs GPT-5\n",
    "- **Set spending limits** in API dashboard\n",
    "- **Cache results:** Don't reprocess the same data\n",
    "\n",
    "### Error Handling\n",
    "- **APIs fail** (network issues, service outages like AWS)\n",
    "- **Implement retries:** Try 2-3 times before giving up\n",
    "- **Save progress frequently:** Don't lose work on crash\n",
    "- **Log errors** for debugging\n",
    "\n",
    "### Rate Limiting\n",
    "- **APIs have limits** depending on tier\n",
    "- **Add delays:** `time.sleep(1)` between requests\n",
    "- **Or use batch APIs:** Submit many requests at once\n",
    "\n",
    "### Security\n",
    "- **Never commit API keys to git!**\n",
    "- **Use environment variables:** `os.getenv('OPENAI_API_KEY')`\n",
    "- **Set spending limits** in dashboard\n",
    "- **Rotate keys** if exposed\n",
    "\n",
    "### Prompt Engineering Basics\n",
    "- **Be specific:** \"Classify as positive, negative, or neutral\" (not just \"sentiment?\")\n",
    "- **Specify format:** \"Return JSON with sentiment and confidence\"\n",
    "- **Test and iterate:** Try on examples, refine prompt"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
