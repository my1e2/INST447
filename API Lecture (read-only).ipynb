{"cells":[{"cell_type":"markdown","metadata":{"id":"L5HKPCDByiDL"},"source":["# INST447: Data Sources and Manipulation\n","## Lecture: Sourcing Data from the Web - HTML Parsing & APIs"]},{"cell_type":"markdown","metadata":{"id":"zdTUvlezyiDN"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"VzQWRctDyiDN"},"source":["## Part 1: Motivation - How Healthy is my Recipe?\n","\n","* **Recap from last week:** We discussed structured data formats like JSON and XML. Unlike flat CSVs, they allow for nested, complex structures, which is how data is often organized on the web.\n","\n","* **Today's Scenario:** Imagine we have a recipe for **Whole Wheat Bread with Nutella**. We want to write a program to determine its nutritional value.\n","\n","* **The Problem:** We need a data source for nutritional information. A quick search leads us to a great resource: [Open Food Facts](https://world.openfoodfacts.org/).\n","\n","* **The Manual Process:** We can go to the website and search for Nutella. We land on this page: [https://world.openfoodfacts.org/product/3017620425035/nutella-ferrero](https://world.openfoodfacts.org/product/3017620425035/nutella-ferrero).\n","\n","    \n","\n","* **The Core Question:** How can we get this information *programmatically*? How do we automate this process for not just Nutella, but potentially thousands of ingredients?"]},{"cell_type":"markdown","metadata":{"id":"ubf76gYmyiDN"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"CY67i6lGyiDO"},"source":["## Part 2: The Direct Approach - Web Scraping & HTML Parsing\n","\n","Our first thought might be: \"Let's just download the webpage and find the data in there.\"\n","\n","This process is called **Web Scraping**.\n","1.  Download the raw HTML content of a webpage.\n","2.  Parse the HTML to navigate its structure.\n","3.  Extract the specific content we need."]},{"cell_type":"markdown","metadata":{"id":"T3aDYOTJyiDO"},"source":["### Step 2.1: Downloading the Webpage\n","\n","We can use the `requests` library in Python to act like a web browser and download the page."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dgUwucPLyiDO"},"outputs":[],"source":["import requests\n","\n","# The URL of the Nutella product page\n","url = 'https://world.openfoodfacts.org/product/3017620425035/nutella-ferrero'\n","\n","# Send a GET request to the URL\n","response = requests.get(url)\n","\n","# Check if the request was successful (status code 200)\n","if response.status_code == 200:\n","    html_content = response.text\n","\n","    # Save it to a file so we can examine it\n","    with open('nutella_page.html', 'w', encoding='utf-8') as f:\n","        f.write(html_content)\n","\n","    print(f\"Successfully downloaded {len(html_content)} characters of HTML.\")\n","    print(\"Saved to: nutella_page.html\")\n","\n","    # Let's look at the first 1000 characters\n","    print(\"\\n--- Start of HTML ---\")\n","    print(html_content[:1000])\n","    print(\"--- End of Snippet ---\")\n","else:\n","    print(f\"Failed to download page. Status code: {response.status_code}\")"]},{"cell_type":"markdown","metadata":{"id":"5akSYE1YyiDP"},"source":["**Observation:** That's... a lot of text. It's a mix of HTML tags (`<!DOCTYPE`, `<html>`, `<head>`), CSS styles, JavaScript, and our actual data somewhere in there. It's not easy to read for a human or a program.\n","\n","**Try this:** Open the saved `nutella_page.html` file in any text editor (VS Code, Notepad, TextEdit, etc.) to see the full complexity of the raw HTML. This will help you appreciate why we need parsing tools!"]},{"cell_type":"markdown","metadata":{"id":"kLuVMIzOyiDP"},"source":["### Step 2.2: Parsing HTML with Beautiful Soup\n","\n","To navigate this complex structure, we need a special tool. `BeautifulSoup` is a Python library that converts a messy HTML string into a structured object that we can search.\n","\n","Let's try to extract useful information from the page."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"myHFDelxyiDP"},"outputs":[],"source":["# You might need to install these libraries first!\n","# !pip install requests beautifulsoup4\n","\n","from bs4 import BeautifulSoup\n","\n","# Parse the HTML content\n","soup = BeautifulSoup(html_content, 'html.parser')\n","\n","# Example 1: Find the product name\n","# By inspecting the page, we find it's in an <h1> tag\n","product_name_element = soup.select_one('h1')\n","product_name = product_name_element.text.strip()  # .strip() removes whitespace\n","\n","print(f\"Product Name: {product_name}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8YPXxszEyiDP"},"outputs":[],"source":["# Example 2: Find the Nutri-Score\n","# This is in a specific section with an id\n","nutriscore_element = soup.select_one('#panel_nutriscore_2023 h4')\n","\n","if nutriscore_element:\n","    nutriscore = nutriscore_element.text.strip()\n","    print(f\"Nutri-Score: {nutriscore}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JgLTcracyiDP"},"outputs":[],"source":["# Example 3: Find nutritional information in a table\n","# We need to find the nutrition table and iterate through its rows\n","nutrition_table = soup.find('table', attrs={'aria-label': 'Nutrition facts'})\n","\n","if nutrition_table:\n","    rows = nutrition_table.find_all('tr')\n","    print(\"Nutritional Information (first 5 items):\")\n","    for row in rows[:5]:  # Show first 5 rows\n","        cells = row.find_all('td')\n","        if cells and len(cells) >= 2:\n","            nutrient = cells[0].text.strip()\n","            value = cells[1].text.strip()\n","            print(f\"  {nutrient}: {value}\")"]},{"cell_type":"markdown","metadata":{"id":"QtIcSPU0yiDP"},"source":["### Discussion: The Problems with Web Scraping\n","\n","We got the data! But is this a good approach?\n","\n","* **üëé Inefficient:** We downloaded over 189,000 characters of HTML, CSS, and JavaScript just to get a few dozen words of nutritional data. This wastes our bandwidth and the website's bandwidth.\n","\n","* **üëé Fragile:** Our code relies on the website's specific HTML structure (e.g., `<h1>` for the name, `#panel_nutriscore_2023 h4` for the Nutri-Score, table with `aria-label='Nutrition facts'`). What if the web developers redesign the page tomorrow? They might change the IDs, restructure the table, or use different elements. **Our scraper would break instantly.**\n","\n","* **üëé Complex:** Notice how we had to use different strategies for each piece of data:\n","    - Simple tag selection for product name\n","    - CSS selector with ID for Nutri-Score  \n","    - Table iteration for nutritional facts\n","    \n","    Every website is structured differently, so we'd need to write custom parsing logic for each site.\n","\n","* **üëé Bad Etiquette:** Scraping can put a heavy load on a website's server, especially if done frequently. It's like sending a fleet of trucks to pick up a single letter. Many websites explicitly forbid aggressive scraping in their `robots.txt` file or Terms of Service."]},{"cell_type":"markdown","metadata":{"id":"XiNP-qUMyiDQ"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"5mWVoSGVyiDQ"},"source":["## Part 3: A Better Way - Using an API\n","\n","Wouldn't it be nice if the website offered a separate, clean, data-only channel for programmers?\n","\n","That's exactly what an **API (Application Programming Interface)** is.\n","\n","> **Analogy:** Scraping a website is like rummaging through a restaurant's kitchen to figure out a recipe. Using an API is like ordering from the menu: you make a specific, structured request and get a specific, structured response.\n","\n","Fortunately, Open Food Facts has a free and open API!"]},{"cell_type":"markdown","metadata":{"id":"KB6QRTnCyiDQ"},"source":["### Step 3.1: Making a Basic API Request\n","\n","The API documentation tells us we can get data for a specific product by using a URL like this:\n","`https://world.openfoodfacts.org/api/v0/product/[barcode].json`\n","\n","Let's try it for Nutella (barcode: 3017620425035)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x7ROmGk2yiDQ"},"outputs":[],"source":["import json\n","\n","api_url = 'https://world.openfoodfacts.org/api/v0/product/3017620425035.json'\n","\n","api_response = requests.get(api_url)\n","\n","# The .json() method from the requests library automatically parses the JSON response\n","data = api_response.json()\n","\n","# Let's print the whole thing to see the structure. It's a dictionary!\n","# We'll use the 'json' library to pretty-print it.\n","print(json.dumps(data, indent=4))"]},{"cell_type":"markdown","metadata":{"id":"IvwKXzLeyiDQ"},"source":["Look at that! It's pure, structured data. No HTML, no CSS, no ads. Just the information we need in a predictable format (JSON).\n","\n","Now, extracting the data is trivial and robust."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IhWOUSv2yiDQ"},"outputs":[],"source":["# The path to the data is predictable based on the JSON structure\n","product_name_api = data['product']['product_name']\n","ingredients_api = data['product']['ingredients_text']\n","countries = data['product']['countries']\n","\n","print(f\"Product Name: {product_name_api}\")\n","print(f\"Ingredients: {ingredients_api}\")\n","print(f\"Sold in: {countries}\")"]},{"cell_type":"markdown","metadata":{"id":"o5j-juTjyiDQ"},"source":["### Step 3.2: Adding Parameters to API Requests\n","\n","Most APIs allow you to customize your request using **parameters** (also called query parameters). These are key-value pairs added to the URL.\n","\n","For example, the Open Food Facts API allows us to search for products. Instead of manually building the URL string, we can pass parameters as a dictionary to `requests.get()`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vSozZeZQyiDQ"},"outputs":[],"source":["# Search API endpoint\n","search_url = 'https://world.openfoodfacts.org/cgi/search.pl'\n","\n","# Parameters as a dictionary - much cleaner than building the URL manually!\n","params = {\n","    'search_terms': 'chocolate',\n","    'page_size': 5,  # Only get 5 results\n","    'json': 1  # Request JSON format\n","}\n","\n","# Pass parameters to requests.get()\n","search_response = requests.get(search_url, params=params)\n","search_data = search_response.json()\n","\n","# Display results\n","print(f\"Found {search_data['count']} products matching 'chocolate'\")\n","print(\"\\nFirst 3 results:\")\n","for i, product in enumerate(search_data['products'][:3], 1):\n","    name = product.get('product_name', 'N/A')\n","    brands = product.get('brands', 'N/A')\n","    print(f\"{i}. {name} ({brands})\")"]},{"cell_type":"markdown","metadata":{"id":"hrDBwNtYyiDQ"},"source":["**What happened?** The `requests` library automatically converted our `params` dictionary into a query string and added it to the URL:\n","\n","```\n","https://world.openfoodfacts.org/cgi/search.pl?search_terms=chocolate&page_size=5&json=1\n","```\n","\n","This is much cleaner than building that string manually, and it handles special characters automatically (like spaces, which get converted to `%20`)."]},{"cell_type":"markdown","metadata":{"id":"NqLdxw6byiDQ"},"source":["### Step 3.3: Limiting Response Data with the `fields` Parameter\n","\n","API responses can be very large, containing hundreds of fields. **Many APIs** (though not all) allow you to request only specific fields to reduce response size and improve performance.\n","\n","The Open Food Facts API supports this through the `fields` parameter - you simply list the fields you want, separated by commas. Let's see how much difference this makes!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DDZB7jYDyiDQ"},"outputs":[],"source":["# First, let's see what a FULL response looks like\n","full_url = 'https://world.openfoodfacts.org/api/v2/product/3017620425035.json'\n","full_response = requests.get(full_url)\n","full_data = full_response.json()\n","\n","print(\"=== Full Response ===\")\n","print(f\"Response size: {len(full_response.text):,} characters\")\n","print(f\"Number of top-level fields: {len(full_data['product'].keys())}\")\n","print(\"\\nThis is a LOT of data - most of which we might not need!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jtpJvwOqyiDQ"},"outputs":[],"source":["# Now, let's request ONLY the fields we actually need\n","limited_url = 'https://world.openfoodfacts.org/api/v2/product/3017620425035.json'\n","params = {\n","    'fields': 'product_name,brands,nutriscore_grade,nutriments'\n","}\n","\n","limited_response = requests.get(limited_url, params=params)\n","limited_data = limited_response.json()\n","\n","print(\"=== Limited Response (with fields parameter) ===\")\n","print(f\"Response size: {len(limited_response.text):,} characters\")\n","print(f\"Number of top-level fields: {len(limited_data['product'].keys())}\")\n","print(f\"\\nReduction: {100 - (len(limited_response.text) / len(full_response.text) * 100):.1f}% smaller!\")\n","print()\n","\n","# Show what we got\n","print(\"Data we requested:\")\n","print(f\"  Product: {limited_data['product']['product_name']}\")\n","print(f\"  Brand: {limited_data['product']['brands']}\")\n","print(f\"  Nutri-Score: {limited_data['product']['nutriscore_grade'].upper()}\")\n","print(f\"  Calories (per 100g): {limited_data['product']['nutriments']['energy-kcal_100g']} kcal\")"]},{"cell_type":"markdown","metadata":{"id":"wIvB-8ivyiDR"},"source":["**Key Takeaway:** Using the `fields` parameter can dramatically reduce response size! In this example, we went from receiving hundreds of fields to just the 4 we needed. This is especially important when:\n","- Making many API requests\n","- Working with slow internet connections\n","- Building mobile apps where data usage matters\n","- Wanting faster response times"]},{"cell_type":"markdown","metadata":{"id":"9IAThvJCyiDR"},"source":["### Step 3.4: API Wrappers - Making APIs Even Easier\n","\n","Using APIs directly with `requests` is already much better than web scraping. But there's an even more convenient approach: **API Wrappers**.\n","\n","**What is an API Wrapper?**\n","An API wrapper (also called a client library or SDK) is a Python package that someone has built to make working with a specific API even easier. Instead of constructing URLs and parsing JSON manually, you call simple Python functions.\n","\n","Think of it this way:\n","- **Raw API:** You're ordering food by calling the restaurant and describing exactly what you want\n","- **API Wrapper:** You're using a delivery app with a nice menu interface\n","\n","Let's see an example using the `openfoodfacts` Python package."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sfCwkh_4yiDR"},"outputs":[],"source":["# First, you would install the wrapper: pip install openfoodfacts\n","# Then you can use it like this:\n","\n","import openfoodfacts\n","\n","# Get product by barcode - much simpler!\n","product = openfoodfacts.products.get_product('3017620425035')\n","\n","if product['status'] == 1:\n","    product_data = product['product']\n","    print(f\"Product Name: {product_data['product_name']}\")\n","    print(f\"Brands: {product_data.get('brands', 'N/A')}\")\n","    print(f\"Nutri-Score: {product_data.get('nutriscore_grade', 'N/A').upper()}\")\n","\n","# Search for products - also simple!\n","search_results = openfoodfacts.products.search('whole wheat bread')\n","print(f\"\\nFound {search_results['count']} products matching 'whole wheat bread'\")\n","print(f\"First result: {search_results['products'][0]['product_name']}\")"]},{"cell_type":"markdown","metadata":{"id":"gmBVb7j7yiDR"},"source":["### Pros and Cons of API Wrappers\n","\n","**Advantages (üëç):**\n","\n","* **üöÄ Easier to Use:** Functions have clear names and parameters. You don't need to memorize URL patterns or JSON structures.\n","* **üìö Better Documentation:** Often includes examples and docstrings that are easier to read than raw API docs.\n","* **üõ°Ô∏è Error Handling:** Good wrappers handle common errors gracefully and provide helpful error messages.\n","* **‚ö° Less Code:** Compare the wrapper code above to manually constructing the URL and parsing JSON.\n","* **üîÑ Automatic Updates:** If the API changes slightly, the wrapper maintainer can update the library without you changing your code.\n","\n","**Disadvantages (üëé):**\n","\n","* **‚è≥ Not Always Available:** Many APIs don't have official wrappers. Third-party wrappers may exist, but quality varies.\n","* **üì¶ Extra Dependency:** You're adding another package to your project. If the wrapper is abandoned, you might have problems.\n","* **üîí Limited Features:** Wrappers might not expose every feature of the API. If you need something advanced, you may still need to use `requests` directly.\n","* **üêõ Additional Layer of Bugs:** The wrapper itself could have bugs, separate from the API.\n","* **üìñ Learning Curve:** You need to learn both the API *and* how the wrapper works.\n","\n","**When to Use Each Approach:**\n","\n","- **Use an API wrapper** when: A well-maintained wrapper exists, and you're doing common operations\n","- **Use `requests` directly** when: No good wrapper exists, you need advanced features, or you want full control\n","- **Never use web scraping** when: An API is available (with or without a wrapper)"]},{"cell_type":"markdown","metadata":{"id":"ejckxmROyiDR"},"source":["### Step 3.5: Being a Good API Citizen\n","\n","APIs are powerful, but they are a shared resource. It's crucial to use them responsibly.\n","\n","* **Authentication:** Many APIs require an **API Key**. This is a unique string that identifies you. It helps the provider track usage and prevent abuse. You usually get one by signing up on their website.\n","\n","* **Rate Limiting:** Don't make too many requests too quickly! API documentation will specify a **rate limit**, like \"100 requests per minute.\" If you exceed this, your key might be temporarily blocked. Always add a small delay (`time.sleep()`) in your code if you're making many requests in a loop.\n","\n","* **Read the Documentation:** The API documentation is your contract with the data provider. It tells you the rules, the available data, and the correct way to ask for it. **Always read it first!**\n","\n","* **Respect the Terms of Service:** Some APIs have restrictions on how you can use the data (e.g., no commercial use, must attribute the source). Always check and follow these rules."]},{"cell_type":"markdown","metadata":{"id":"SnP5BArhyiDR"},"source":["## Part 4: Conclusion & The Data Sourcing Hierarchy\n","\n","We've explored multiple ways to get data from the web. This leads to a clear hierarchy of preference when you're looking for a data source:\n","\n","**ü•á Level 1: API with a good wrapper library**\n","   * **Pros:** Most convenient, clean code, often includes helpful features\n","   * **Example:** `openfoodfacts.products.get_product('123')`\n","   * **Use when:** A well-maintained wrapper exists for your API\n","\n","**ü•à Level 2: Well-documented API (using `requests`)**\n","   * **Pros:** Efficient, stable, robust, and the intended method of access\n","   * **Example:** `requests.get('https://api.example.com/products/123')`\n","   * **Use when:** No wrapper exists, or you need full control\n","\n","**ü•â Level 3: Structured Data Files (e.g., a direct CSV/JSON download link)**\n","   * **Pros:** Data is already in a clean format\n","   * **Cons:** Might not be the most up-to-date data\n","\n","**üèÖ Level 4: Web Scraping (Last Resort)**\n","   * **Pros:** Can get data from almost any website\n","   * **Cons:** Inefficient, fragile, complex, and ethically ambiguous\n","   * **Use only when:** No API or data download is available, and you have permission\n","\n","**Key Takeaway:** Always look for an API first! And if an API exists, check if there's a wrapper to make your life even easier.\n","\n","By understanding these methods, you are now equipped to programmatically source data from the vast resources of the internet, a crucial skill for any data scientist."]}],"metadata":{"kernelspec":{"display_name":"ds","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}